{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dfa2b6",
   "metadata": {},
   "source": [
    "# Primeras nociones de deep-learning y redes neuronales \n",
    "## Ejemplo chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bd059",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip list #para saber que tengo instalado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab544c10",
   "metadata": {},
   "source": [
    "import pickle  #https://docs.python.org/es/3/library/pickle.html  # guardar modelos (o objetos) como secuencia de bytes (unos y ceros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9b61c",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "\n",
    "- que es en palabras sencillas\n",
    " https://www.incentro.com/es-ES/blog/que-es-tensorflow\n",
    " \n",
    "- link de la libreria\n",
    " https://www.tensorflow.org/?gclid=Cj0KCQiA4OybBhCzARIsAIcfn9nLmib7_Bs2vXmVsx1nI5y3gF07NhydNa0Tfgf6rxdBTf1zZJiA08kaAt7XEALw_wcB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c4e9a",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras\n",
    "\n",
    "- tensorflow vs keras, que son?\n",
    "https://nodd3r.com/blog/keras-vs-tensorflow-vs-pytorch-diferencias-clave-entre-frameworks-de-deep-learning\n",
    "\n",
    "-   que es una api\n",
    "https://www.sydle.com/es/blog/api-6214f68876950e47761c40e7/\n",
    "Una API es una especie de puente que conecta diversos tipos de software o aplicaciones y puede crearse en varios lenguajes de programación. Además de un buen desarrollo, una API debe tener una documentación clara y objetiva para poder facilitar su implementación.\n",
    "\n",
    "-  link libreria\n",
    "https://keras.io/examples/   ESTO LES SIRVE PARA LOS PROYECTOS !!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b0f51",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "\n",
    "- que es en palabras sencillas \n",
    "https://code.tutsplus.com/es/tutorials/introducing-the-natural-language-toolkit-nltk--cms-28620\n",
    "\n",
    "- link de la libreria\n",
    "https://www.nltk.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d02406",
   "metadata": {},
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('spanish_grammars')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('stopwords') # faltaaaaa\n",
    "nltk.download('omw-1.4') #faltaaaa\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abeb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def guardar_json(datos):\n",
    "    archivo=open(\"intents.json\",\"w\")\n",
    "    json.dump(datos,archivo,indent=4)\n",
    "\n",
    "#intents: grupos de conversaciones tipicas para nuestro objetivo\n",
    "# patterns: posibles interacciones con el usuario\n",
    "\n",
    "#dic={\"intents:[[{\"key\":[\"valores\"]}],\"dic2\"]}\n",
    "\n",
    "\n",
    "biblioteca={\"intents\":\n",
    "            [\n",
    "                {\"tag\":\"saludos\",\n",
    "                 \"patterns\":[\"hola\",\n",
    "                             \"buenos dias\",\n",
    "                             \"buenas tardes\",\n",
    "                             \"buenas noches\",\n",
    "                             \"como estas\",\n",
    "                             \"hay alguien ahi?\",\n",
    "                             \"hey\",\n",
    "                             \"saludos\",\n",
    "                             \"que tal\"                      \n",
    "                             ],\n",
    "                 \"responses\":[\"hola soy SIC-BOT , en que puedo ayudarte\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]\n",
    "                 },\n",
    "                \n",
    "                {\"tag\":\"despedidas\",\n",
    "                 \"patterns\":[\"chao\",\n",
    "                             \"adios\",\n",
    "                             \"hasta luego\",\n",
    "                             \"nos vemos\",\n",
    "                             \"bye\",\n",
    "                             \"hasta pronto\",\n",
    "                             \"hasta la proxima\"\n",
    "                             ],\n",
    "                 \"responses\":[\"hasta luego, tenga un buen dia\",\n",
    "                 \"ha sido un placer, vuelva pronto\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]\n",
    "                 },\n",
    "                {\"tag\":\"agradecimientos\",\n",
    "                 \"patterns\":[\"gracias\",\n",
    "                             \"muchas gracias\",\n",
    "                             \"mil gracias\",\n",
    "                             \"muy amable\",\n",
    "                             \"se lo agradezco\",\n",
    "                             \"fue de ayuda\",\n",
    "                             \"gracias por la ayuda\",\n",
    "                             \"muy agradecido\",\n",
    "                             \"gracias por su tiempo\",\n",
    "                             \"ty\"\n",
    "                            ],\n",
    "                 \"responses\":[\"de nada\",\n",
    "                              \"feliz por ayudarlo\",\n",
    "                              \"gracias a usted\",\n",
    "                              \"estamos para servirle\",\n",
    "                              \"fue un placer\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]\n",
    "                },\n",
    "                {\"tag\":\"norespuesta\",\n",
    "                 \"patterns\":[\"\"],\n",
    "                 \"responses\":[\"no se detecto una respuesta\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]                    \n",
    "                }\n",
    "            ] \n",
    "        }\n",
    "guardar_json(biblioteca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a56a78c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5944\\2206575623.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "ignore_words=[\"?\",\"¿\",\"!\",\"¡\"]\n",
    "data_file= open(\"intents.json\").read() #aqui cargo el archivo en formato json\n",
    "intents = json.loads(data_file) # aqui convierto el archivo json a diccionario\n",
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7d129",
   "metadata": {},
   "source": [
    "Cuando trabajamos con datos de texto, necesitamos realizar varios preprocesos en los datos antes de hacer un machine\n",
    "learning o un modelo de Deep learning.\n",
    "\n",
    "Según los requisitos, necesitamos aplicar varias operaciones para preprocesar los datos.\n",
    "\n",
    "La creación de tokens es lo más básico y lo primero que puedes hacer con los datos de texto. \n",
    "\n",
    "La creación de tokens es el proceso de dividir todo el texto en partes pequeñas, como palabras.\n",
    "\n",
    "Aquí iteramos a través de los patrones y tokenizamos la oración usando la función nltk.word_tokenize ()\n",
    "y agregamos cada palabra en la lista de palabras. También creamos una lista de clases para nuestras etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd877a3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words=[]\n",
    "classes=[]\n",
    "documents=[]\n",
    "\n",
    "for intent in intents[\"intents\"]: #accedo a la lista de diccionarios\n",
    "    for pattern in intent[\"patterns\"]: # accedo a la lista de palabraas\n",
    "        \n",
    "        \n",
    "        #tokenizar cada palabra\n",
    "        \n",
    "        w=nltk.word_tokenize(pattern) #separamos las oraciones palabra por palabra y guardamos cada palabra como token\n",
    "        words.extend(w)\n",
    "        \n",
    "        #agrego un array de documentos\n",
    "        documents.append((w,intent[\"tag\"]))\n",
    "        \n",
    "        #añadimos clases  a nuestra lista de clases\n",
    "        if intent[\"tag\"] not in classes:\n",
    "            classes.append(intent[\"tag\"])\n",
    "            \n",
    "print(words)\n",
    "\n",
    "print(\"\\n ####################################################################################################### \\n\")\n",
    "\n",
    "print(documents)\n",
    "\n",
    "print(\"\\n ####################################################################################################### \\n\")\n",
    "\n",
    "print(classes)\n",
    "\n",
    "\n",
    "\n",
    "#recorderis\n",
    "#append añade todo en el mismo formato, ejemplo si append([lista]) lo agrega como lista\n",
    "#extend añade los elementos uno por uno , ejemplo si extend([lista]) agrega cada elemento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf642108",
   "metadata": {},
   "source": [
    "Ahora lematizaremos cada palabra y eliminaremos las palabras duplicadas de la lista.\n",
    "Lematizar es el proceso de convertir una palabra en su forma de lema y luego crear un archivo pickle para almacenar\n",
    "los objetos de Python que usaremos al predecir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf16ef4",
   "metadata": {},
   "source": [
    "# CAMBIO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9711bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# ejemplo de lematizacion: https://es.wikipedia.org/wiki/Lematización\n",
    "# lema -> palabra completa  : categoria\n",
    "# casa -> casas             : plurales\n",
    "# dije -> diré,dijeramos    :tiempos verbales\n",
    "\n",
    "#La lematización es el proceso de agrupar las diferentes formas flexionadas de una \n",
    "                                #palabra para que puedan ser analizadas como un único elemento\n",
    "\n",
    "\n",
    "#base de datos, verdad absoluta de las palabras -> REFERENCIA\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "\n",
    "print(\"esta lista words seran las palabras LEMATIZADAS de REFERENCIA\")\n",
    "print(words)\n",
    "\n",
    "\n",
    "pickle.dump(words,open(\"words.pkl\",\"wb\"))\n",
    "pickle.dump(classes,open(\"classes.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7205b",
   "metadata": {},
   "source": [
    "### teoria\n",
    "\n",
    "https://www.nltk.org/book_1ed/ch03.html\n",
    "\n",
    "   Method               Functionality\n",
    "- s.find(t)=\t         index of first instance of string t inside s (-1 if not found)\n",
    "- s.rfind(t)=\t     index of last instance of string t inside s (-1 if not found)\n",
    "- s.index(t)=\t     like s.find(t) except it raises ValueError if not found\n",
    "- s.rindex(t)=\t     like s.rfind(t) except it raises ValueError if not found\n",
    "- s.join(text)=\t     combine the words of the text into a string using s as the glue\n",
    "- s.split(t)=\t     split s into a list wherever a t is found (whitespace by default)\n",
    "- s.splitlines()=\t split s into a list of strings, one per line\n",
    "- s.lower()=\t         a lowercased version of the string s\n",
    "- s.upper()=\t         an uppercased version of the string s\n",
    "- s.title()=\t         a titlecased version of the string s\n",
    "- s.strip()=\t         a copy of s without leading or trailing whitespace\n",
    "- s.replace(t, u)=\t replace instances of t with u inside s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662debb",
   "metadata": {},
   "source": [
    "Ahora, crearemos los datos de entrenamiento en los que proporcionaremos la entrada y la salida.\n",
    "Nuestra entrada será el patrón y la salida será la clase a la que pertenece nuestro patrón de entrada. \n",
    "Pero la computadora no entiende el texto, así que convertiremos el texto en números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd3232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparacion para la formacion de nuestra red neuronal \n",
    "\n",
    "\n",
    "#en esta primera parte de nuestra red neuronal estamos haciendo un clasificador, words en tags\n",
    "\n",
    "training=[]\n",
    "output_empty=[0]*len(classes)# creamos una matriz del numero de patterns con valor inicial 0\n",
    "                            # creamos una matriz que tenga tantas columnas como classes\n",
    "\n",
    "for doc in documents: #en doc esta la raw_data -> datos sin procesar\n",
    "#doc[0] -> tokens, o palabras\n",
    "#doc[1] -> tag -> clase\n",
    "    \n",
    "    \n",
    "    #bag of words\n",
    "    bag=[]\n",
    "    #lista de tokens\n",
    "    pattern_words=doc[0]# doc[0] es la lista de palabras\n",
    "    # lematizacion del token\n",
    "#CAMBIO!!!!    pattern_words= [lemmatizer.lemmatize(word.lower()) for word in pattern_words  if word not in ignore_words ]\n",
    "    pattern_words= [stemmer.stem(word.lower()) for word in pattern_words  if word not in ignore_words ]\n",
    "    \n",
    "    \n",
    "    # si la palabra coincide introduzco 1, en caso contrario 0\n",
    "    \n",
    "    for palabra in words:\n",
    "        bag.append(1) if palabra in pattern_words else bag.append(0) \n",
    "        #si la palabra de referencia esta dentro de pattern_words ponga 1\n",
    "        #print(bag)\n",
    "    \n",
    "    output_row =list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1 #doc en la posicion 1 es el pattern\n",
    "                #busca en que posicion esta el tag y pone un 1 en esa posicion del output_row\n",
    "                #ejemplo si es saludo pone [1,0,0,0]\n",
    "    \n",
    "    training.append([bag,output_row])\n",
    "    #print(output_row)\n",
    "\n",
    "#print(training)\n",
    "training=np.array(training) # cambiamos la lista de listas a un formato numpy.array\n",
    "\n",
    "print(f\"tendremos como entradas {len(training[0][0])} columnas\")\n",
    "#print(f\"tendremos como entradas {} columnas\".format(len(training[0][0])))      \n",
    "\n",
    "print(f\"tendremos como salidas {len(training[0][1])} columnas\")      \n",
    "print()      \n",
    "print(training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b8e454",
   "metadata": {},
   "source": [
    "que hace todo este codigo anterior tan horroroso:\n",
    "- estamos ENCRIPTANDO palabras a listas de 1 y 0\n",
    "\n",
    "convierte palabras en \"booleano\"(lista de 1 y 0) y les asigna una categoria \n",
    "\n",
    "para asignal la categoria busca si esta en la lista de referencia\n",
    "\n",
    "como son las categorias:\n",
    "     [1,0,0,0]=saludos\n",
    "     [0,1,0,0]=despedidas\n",
    "     [0,0,1,0]=agradecimientos\n",
    "     [0,0,0,1]=norespuesta\n",
    "\n",
    "    \n",
    "forma pares:    \n",
    "\n",
    " hola   :[1, 0, 0, 0, 0, 0, ... [1,0,0] -> 10000 : saludo\n",
    " buenos :[0, 1, 1, 0, 0, 0, ... [1,0,0] -> 01100 : saludo\n",
    "         \n",
    "estos pares son la data que vamos a mandar a la IA para poder crear el modelo\n",
    " \n",
    "     52 columnas     4 columnas\n",
    " data=entradas     target=salidas\n",
    "c1,c2,c3,c4,c5,c6...s1,s2,s3,s4\n",
    "1  0  0  0  0  0    1  0  0   0  -> saludo    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3155f9",
   "metadata": {},
   "source": [
    "# Hasta ahi la clase anterior con los cambios en el lematizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aecb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en la clase anterior habiamos creado el training, ahora separamos\n",
    "# crear los conjuntos de entrenamiento  y prueba, x_test, x_train (entradas) , y_test , y_train (salidas)\n",
    "        \n",
    "#x_train= training[0] asi seria en listas     \n",
    "x_train= list(training[:,0]) #asi porque estamos en formato numpy.array ||| training[inicio:fin,index]\n",
    "y_train= list(training[:,1])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ca5fa",
   "metadata": {},
   "source": [
    "Tenemos nuestros datos de entrenamiento listos, ahora construiremos una red neuronal profunda que tiene 3 capas.\n",
    "https://www.youtube.com/watch?v=IQMoglp-fBk # como funciona una red neuronal de 3 capas\n",
    "\n",
    "Usamos la API secuencial de Keras para esto.\n",
    "Después de entrenar el modelo durante 300 épocas, logramos una precisión del 100% en nuestro modelo.\n",
    "Guardemos el modelo como “chatbot_model.h5”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creacion del modelo\n",
    "\n",
    "model = Sequential()\n",
    "#https://keras.io/guides/sequential_model/\n",
    "#https://www.youtube.com/watch?v=1M0x9k-rMDM\n",
    "\n",
    "#añadimos capas a la red\n",
    "model.add(Dense(128, input_shape=(len(x_train[0]),), activation='relu')) #añadimos 1 capa: entrada de datos\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64,activation='relu')) #capa oculta -> aprendizaje\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(y_train[0]),activation='softmax')) # capa de salida toma de desiciones\n",
    "\n",
    "#El (DROPOUT) es una técnica en la que se ignoran neuronas seleccionadas al azar \n",
    "#durante el entrenamiento. Se \"descartan\" aleatoriamente. \n",
    "#Esto significa que su contribución a la activación de las neuronas descendentes \n",
    "#se elimina temporalmente en el paso hacia delante, y cualquier actualización de\n",
    "#peso no se aplica a la neurona en el paso hacia atrás.\n",
    "\n",
    "\n",
    "# SGD es un optimizador estocástico de descenso gradiente. \n",
    "# Incluye soporte para momentum, decaimiento de la tasa de aprendizaje y momentum Nesterov.\n",
    "\n",
    "# SGD optimiza los parametros\n",
    "\n",
    "\n",
    "sgd=SGD(learning_rate=0.01,decay=1e-6,momentum=0.9,nesterov=True) \n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=sgd,metrics=[\"accuracy\"])\n",
    "\n",
    "#le mando los datos de train para que entrene y aprenda\n",
    "#fit ajusta los datos para crear un modelo (Sequential de 3 capas) que pueda predecir los datos\n",
    "\n",
    "hist=model.fit(np.array(x_train),np.array(y_train),epochs=300,batch_size=5,verbose=1)\n",
    "model.save(\"chatbot_model.h5\",hist)\n",
    "print(\"modelo creado\")\n",
    "\n",
    "# en deep learning el sistema aprende cada epoch , en el epoch=1 NO SABE NADA\n",
    "# en el epoch 267, ya es brillante!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283cdde6",
   "metadata": {},
   "source": [
    "# en la vida real\n",
    "archivo diferente, enviamos a la empresa  los archivos intents, words, classes, modelo y el ejecutable del programa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9962a5",
   "metadata": {},
   "source": [
    "Predecir la respuesta (interfaz gráfica de usuario) Para predecir las oraciones y obtener una respuesta del usuario que nos permita crear un nuevo archivo “chatapp.py”. Cargaremos el modelo entrenado y luego usaremos una interfaz gráfica de usuario que predecirá la respuesta del bot. El modelo solo nos dirá la clase a la que pertenece, por lo que implementaremos algunas funciones que identificarán la clase y luego recuperaremos una respuesta aleatoria de la lista de respuestas.\n",
    "\n",
    "Nuevamente importamos los paquetes necesarios y cargamos los archivos pickle “words.pkl” y “classes.pkl” que hemos creado cuando entrenamos nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800714a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, json,pickle\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tensorflow.keras.models import load_model\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "model=load_model(\"chatbot_model.h5\")\n",
    "intents= json.loads(open(\"intents.json\").read())\n",
    "words=pickle.load(open(\"words.pkl\",\"rb\"))\n",
    "classes=pickle.load(open(\"classes.pkl\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc1591",
   "metadata": {},
   "source": [
    "Para predecir la clase, necesitaremos proporcionar información de la misma manera que lo hicimos durante el entrenamiento. Entonces crearemos algunas funciones que realizarán el preprocesamiento de texto y luego predecirán la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c24507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesamiento de la entrada del usuario!!\n",
    "\n",
    "#todo lo que vaya con sentence es lo que ingreso el usuario\n",
    "\n",
    "# en los siguientes 2 metodos  voy a convertir las palabras que ingresa el usuario al formato\n",
    "# de entrenamiento que usamos anteriormente, osea, convertir las palabras en una lista de 1 y 0 \n",
    "# en un array donde palabra,tag -> [1,0,0,1,1],[1,0,0,0]\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenizar la oracion\n",
    "    sentence_words=nltk.word_tokenize(sentence) # tokenizamos\n",
    "    sentence_words=[stemmer.stem(word.lower()) for word in sentence_words] #lematizamos\n",
    "    return sentence_words\n",
    "\n",
    "\n",
    "def bow (sentence,words,show_details=True): #lazo entre lo que ingreso el usuario tokenizado y la referencia \n",
    "    sentence_words=clean_up_sentence(sentence)\n",
    "    \n",
    "    bag=[0]*len(words)\n",
    "    \n",
    "    for i in sentence_words:\n",
    "        for j,w in enumerate(words):\n",
    "            if w==i: # asigna 1 si la palabra actual está en la posición del vocabulario \n",
    "                bag[j]=1\n",
    "                if show_details:\n",
    "                    print(\"encontrado en la bolsa: \",w)\n",
    "    return (np.array(bag))\n",
    "\n",
    "# ahora si utilizo el modelo para predecir que tipo de palabra es\n",
    "\n",
    "\n",
    "def predict_class(sentence,model):\n",
    "    # filtrar las predicciones  por debajo del umbral\n",
    "    p = bow(sentence,words,show_details=False) # retorno del bag # p porque es el preprocesamiento\n",
    "    res = model.predict(np.array([p]))[0] # res es la eficacia, o probabilidad de que la palabra sea de algun tipo\n",
    "    #model.predict me retorna el % eficacia  , ejm 60% saludo\n",
    "    # [0] es la palabra , [1] es el tag\n",
    "    \n",
    "    \n",
    "    ERROR_THRESHOLD=0.25 #UMBRAL\n",
    "    \n",
    "    \n",
    "    # a results le llega  [1,0,0,0]\n",
    "    results= [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD] \n",
    "    # si la probabilidad es > 25% determinela como resultado correcto\n",
    "    \n",
    "    #r[0]= tag\n",
    "    #r[1]= probabilidad\n",
    "    \n",
    "    #ordenar por peso de la probabilidad\n",
    "    results.sort(key=lambda x: x[1], reverse=True)  \n",
    "    #ordena de mayor a menor la probabilidad de que el resultado sea acertado\n",
    "    return_list = []    \n",
    "    for r in results:   \n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})   \n",
    "    print(\"print de return list: \", return_list)  ##  me dice de que tipo es y cual es la probabilidad de que sea correcto\n",
    "    return return_list \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f25836",
   "metadata": {},
   "source": [
    "Despues de predecir la clase, obtendremos una respuesta aleatoria de la lista de intents segun lo que haya ingresado el usuario en sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed9327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_response(ints,intents_json): # obtiene una respuesta aleatoria segun el ints correspondiente a lo que ingreso el usuario\n",
    "    tag= ints[0][\"intent\"] # obtenemos cual era el tag  segun lo que ingreso el usuario ints[0]\n",
    "    list_of_intents=intents_json[\"intents\"] # sacamos la lista de intents de referencia\n",
    "    \n",
    "    for i  in list_of_intents: \n",
    "        if (i[\"tag\"]==tag): #miramos donde coincide el tag del sentence con la referencia\n",
    "            result= random.choice(i[\"responses\"]) # random.choice, toma un elemento aleatorio de la lista\n",
    "            break\n",
    "    return result\n",
    "    \n",
    "# este es el metodo principal, aqui nace todo\n",
    "def chatbot_response(text): \n",
    "    ints=predict_class(text,model) #ints es el intents que creamos a apartir de lo que ingreso el usuario\n",
    "    res=get_response(ints,intents)# intents es el json de referencia\n",
    "    return res\n",
    "    \n",
    "    \n",
    "######################## SOLO PARA PROBARLO EN CONSOLA ##########################\n",
    "# \"main\"\n",
    "texto_us=\"\" # lo que ingresa el usuario\n",
    "print(\" bienvenido, para salir  escriba salir \\n\")\n",
    "\n",
    "while texto_us!=\"salir\":\n",
    "    texto_us=input()\n",
    "    res=chatbot_response(texto_us)\n",
    "    print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca47f64",
   "metadata": {},
   "source": [
    "# Prueba de escritorio\n",
    "texto_us = buenas tardes\n",
    "\n",
    "chatbot_response:\n",
    "\tpredict_class:\n",
    "\t\tbow:\n",
    "\t\t\tclean_up_sentence:\n",
    "\t\t\treturn=buen,tarde\t\n",
    "\t\treturn= bag\n",
    "\t\t\n",
    "\t\tmodel.predict\n",
    "\treturn = return_list (tag de lo que ingreso el usuario)\n",
    "\tints= saludo\n",
    "\n",
    "\tget_response:\n",
    "\t\treturn= result =  \"hola soy SIC-BOT , en que puedo ayudarte\"\n",
    "res= \"hola soy SIC-BOT , en que puedo ayudarte\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1f02f",
   "metadata": {},
   "source": [
    "Ahora desarrollaremos una interfaz gráfica de usuario. Usemos la biblioteca Tkinter la cual posee herramientas útiles para GUI(graphic user interface. Tomaremos el mensaje de entrada del usuario y luego usaremos las funciones auxiliares que hemos creado para obtener la respuesta del bot y mostrarla en la GUI.\n",
    "https://docs.python.org/3/library/tkinter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52179c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear una interfaz grafica con tkinter\n",
    "\n",
    "import tkinter\n",
    "from tkinter import * # importa todo\n",
    "\n",
    "# * representa todas las funciones y módulos incorporados en la librería tkinter.\n",
    "#Al importar todas las funciones y métodos, podemos utilizar las funciones o métodos incorporados\n",
    "#en una aplicación particular sin importarlos implícitamente.\n",
    "\n",
    "def send():\n",
    "    msg=entrybox.get(\"1.0\",\"end-1c\").strip() # sentence\n",
    "                                    #strip limpia la entrada, quita el espacio antes y despues\n",
    "    entrybox.delete(\"0.0\",END)\n",
    "    if msg!=\"\":\n",
    "        chatlog.config(state=NORMAL)# habilita el chatlog\n",
    "        chatlog.insert(END,\"USUARIO:\" + msg +\"\\n\\n\" )\n",
    "        chatlog.config(foreground=\"#442265\",font=(\"verdana\",12))\n",
    "        res=chatbot_response(msg)\n",
    "        chatlog.insert(END,\"SIC-BOT:\" + res +\"\\n\\n\" )\n",
    "        chatlog.config(state=DISABLED)\n",
    "        chatlog.yview(END)\n",
    "\n",
    "base=Tk() # creamos el objeto\n",
    "base.title(\"SIC-BOT\") # ponemos un titulo\n",
    "base.geometry(\"400x500\") # ponemos el tamaño\n",
    "base.resizable(width=FALSE,height=FALSE)\n",
    "\n",
    "# crear la ventana de chat\n",
    "# primero creamos el espacio de texto donde se mostrará el historial\n",
    "chatlog= Text(base,bd=0,bg=\"white\",height=\"8\",width=\"50\",font=\"Arial\") # creamos una ventana de texto\n",
    "chatlog.config(state=DISABLED)\n",
    "\n",
    "# vincular una barra de desplazamiento\n",
    "scrollbar = Scrollbar(base,command=chatlog.yview, cursor=\"heart\")\n",
    "chatlog[\"yscrollcommand\"]=scrollbar.set\n",
    "\n",
    "# casilla para introducir el mensaje\n",
    "entrybox=Text(base,bd=0,bg=\"white\",height=\"5\",width=\"26\",font=\"Arial\")\n",
    "\n",
    "# boton para enviar el mensaje\n",
    "sendbutton= Button(base, font=(\"Verdana\",12,'bold'),\n",
    "                    text=\"Enviar\", width=\"10\", height=5, bd=0, bg=\"#32de97\",\n",
    "                    activebackground=\"#3c9d9b\",fg='#ffffff', command= send )\n",
    "\n",
    "# invoco, coloco los componentes en la pantalla\n",
    "scrollbar.place(x=376,y=6, height=386)\n",
    "chatlog.place(x=6,y=6, height=386, width=370)\n",
    "entrybox.place(x=6, y=401, height=90, width=265)\n",
    "sendbutton.place(x=272, y=401, height=90)\n",
    "base.mainloop()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed56bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce572a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip list #para saber que tengo instalado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a8088",
   "metadata": {},
   "source": [
    "keras                         2.10.0\n",
    "Keras-Preprocessing           1.1.2\n",
    "nltk                          3.7\n",
    "numpy                         1.21.5\n",
    "tensorflow                    2.10.1\n",
    "tensorflow-estimator          2.10.0\n",
    "tensorflow-io-gcs-filesystem  0.27.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluar la eficiencia del modelo:\n",
    "    \n",
    "import scikitplot as skplt\n",
    "\n",
    "skplt.estimators.plot_learning_curve(model,x_train,y_train,figsize=(6,6))\n",
    "plt.show()\n",
    "   \n",
    "\n",
    "    \n",
    "skplt.metrics.plot_confusion_matrix(y_test,pred,figsize=(8,6))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dfa2b6",
   "metadata": {},
   "source": [
    "# Primeras nociones de deep-learning y redes neuronales \n",
    "## Ejemplo chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bd059",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip list #para saber que tengo instalado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab544c10",
   "metadata": {},
   "source": [
    "import pickle  #https://docs.python.org/es/3/library/pickle.html  # guardar modelos (o objetos) como secuencia de bytes (unos y ceros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9b61c",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "\n",
    "- que es en palabras sencillas\n",
    " https://www.incentro.com/es-ES/blog/que-es-tensorflow\n",
    " \n",
    "- link de la libreria\n",
    " https://www.tensorflow.org/?gclid=Cj0KCQiA4OybBhCzARIsAIcfn9nLmib7_Bs2vXmVsx1nI5y3gF07NhydNa0Tfgf6rxdBTf1zZJiA08kaAt7XEALw_wcB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c4e9a",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras\n",
    "\n",
    "- tensorflow vs keras, que son?\n",
    "https://nodd3r.com/blog/keras-vs-tensorflow-vs-pytorch-diferencias-clave-entre-frameworks-de-deep-learning\n",
    "\n",
    "-   que es una api\n",
    "https://www.sydle.com/es/blog/api-6214f68876950e47761c40e7/\n",
    "Una API es una especie de puente que conecta diversos tipos de software o aplicaciones y puede crearse en varios lenguajes de programación. Además de un buen desarrollo, una API debe tener una documentación clara y objetiva para poder facilitar su implementación.\n",
    "\n",
    "-  link libreria\n",
    "https://keras.io/examples/   ESTO LES SIRVE PARA LOS PROYECTOS !!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b0f51",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "\n",
    "- que es en palabras sencillas \n",
    "https://code.tutsplus.com/es/tutorials/introducing-the-natural-language-toolkit-nltk--cms-28620\n",
    "\n",
    "- link de la libreria\n",
    "https://www.nltk.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ad5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('spanish_grammars')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('stopwords') # faltaaaaa\n",
    "nltk.download('omw-1.4') #faltaaaa\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0abeb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def guardar_json(datos):\n",
    "    archivo=open(\"intents.json\",\"w\")\n",
    "    json.dump(datos,archivo,indent=4)\n",
    "\n",
    "#intents: grupos de conversaciones tipicas para nuestro objetivo\n",
    "# patterns: posibles interacciones con el usuario\n",
    "\n",
    "#dic={\"intents:[[{\"key\":[\"vlores\"]}],\"dic2\"]}\n",
    "\n",
    "\n",
    "biblioteca={\"intents\":\n",
    "            [\n",
    "                {\"tag\":\"saludos\",\n",
    "                 \"patterns\":[\"hola\",\n",
    "                             \"buenos dias\",\n",
    "                             \"buenas tardes\",\n",
    "                             \"buenas noches\",\n",
    "                             \"como estas\",\n",
    "                             \"hay alguien ahi?\",\n",
    "                             \"hey\",\n",
    "                             \"saludos\",\n",
    "                             \"que tal\"                      \n",
    "                             ],\n",
    "                 \"responses\":[\"hola soy SIC-BOT , en que puedo ayudarte\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]\n",
    "                 },\n",
    "                \n",
    "                {\"tag\":\"despedidas\",\n",
    "                 \"patterns\":[\"chao\",\n",
    "                             \"adios\",\n",
    "                             \"hasta luego\",\n",
    "                             \"nos vemos\",\n",
    "                             \"bye\",\n",
    "                             \"hasta pronto\",\n",
    "                             \"hasta la proxima\"\n",
    "                             ],\n",
    "                 \"responses\":[\"hasta luego, tenga un buen dia\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]\n",
    "                 },\n",
    "                {\"tag\":\"agradecimientos\",\n",
    "                 \"patterns\":[\"gracias\",\n",
    "                             \"muchas gracias\",\n",
    "                             \"mil gracias\",\n",
    "                             \"muy amable\",\n",
    "                             \"se lo agradezco\",\n",
    "                             \"fue de ayuda\",\n",
    "                             \"gracias por la ayuda\",\n",
    "                             \"muy agradecido\",\n",
    "                             \"gracias por su tiempo\",\n",
    "                             \"ty\"\n",
    "                            ],\n",
    "                 \"responses\":[\"de nada\",\n",
    "                              \"feliz por ayudarlo\",\n",
    "                              \"gracias a usted\",\n",
    "                              \"estamos para servirle\",\n",
    "                              \"fue un placer\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]\n",
    "                },\n",
    "                {\"tag\":\"norespuesta\",\n",
    "                 \"patterns\":[],\n",
    "                 \"responses\":[\"no se detecto una respuesta\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]                    \n",
    "                }\n",
    "            ] \n",
    "        }\n",
    "guardar_json(biblioteca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a56a78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'saludos',\n",
       "   'patterns': ['hola',\n",
       "    'buenos dias',\n",
       "    'buenas tardes',\n",
       "    'buenas noches',\n",
       "    'como estas',\n",
       "    'hay alguien ahi?',\n",
       "    'hey',\n",
       "    'saludos',\n",
       "    'que tal'],\n",
       "   'responses': ['hola soy SIC-BOT , en que puedo ayudarte'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'despedidas',\n",
       "   'patterns': ['chao',\n",
       "    'adios',\n",
       "    'hasta luego',\n",
       "    'nos vemos',\n",
       "    'bye',\n",
       "    'hasta pronto',\n",
       "    'hasta la proxima'],\n",
       "   'responses': ['hasta luego, tenga un buen dia'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'agradecimientos',\n",
       "   'patterns': ['gracias',\n",
       "    'muchas gracias',\n",
       "    'mil gracias',\n",
       "    'muy amable',\n",
       "    'se lo agradezco',\n",
       "    'fue de ayuda',\n",
       "    'gracias por la ayuda',\n",
       "    'muy agradecido',\n",
       "    'gracias por su tiempo',\n",
       "    'ty'],\n",
       "   'responses': ['de nada',\n",
       "    'feliz por ayudarlo',\n",
       "    'gracias a usted',\n",
       "    'estamos para servirle',\n",
       "    'fue un placer'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'norespuesta',\n",
       "   'patterns': [],\n",
       "   'responses': ['no se detecto una respuesta'],\n",
       "   'context': ['']}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "    #stemmer=LancasterStemmer() #opcional esp\n",
    "lemmatizer=WordNetLemmatizer()#La lematización es el proceso de agrupar las diferentes formas flexionadas de una \n",
    "                                #palabra para que puedan ser analizadas como un único elemento\n",
    "    \n",
    "# ejemplo de lematizacion: https://es.wikipedia.org/wiki/Lematización\n",
    "# lema -> palabra completa  : categoria\n",
    "# casa -> casas             : plurales\n",
    "# dije -> diré,dijeramos    :tiempos verbales\n",
    "\n",
    "\n",
    "ignore_words=[\"?\",\"¿\",\"!\",\"¡\"]\n",
    "data_file= open(\"intents.json\").read() #aqui cargo el archivo en formato json\n",
    "intents = json.loads(data_file) # aqui convierto el archivo json a diccionario\n",
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7d129",
   "metadata": {},
   "source": [
    "Cuando trabajamos con datos de texto, necesitamos realizar varios preprocesos en los datos antes de hacer un machine\n",
    "learning o un modelo de Deep learning.\n",
    "\n",
    "Según los requisitos, necesitamos aplicar varias operaciones para preprocesar los datos.\n",
    "\n",
    "La creación de tokens es lo más básico y lo primero que puedes hacer con los datos de texto. \n",
    "\n",
    "La creación de tokens es el proceso de dividir todo el texto en partes pequeñas, como palabras.\n",
    "\n",
    "Aquí iteramos a través de los patrones y tokenizamos la oración usando la función nltk.word_tokenize ()\n",
    "y agregamos cada palabra en la lista de palabras. También creamos una lista de clases para nuestras etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd877a3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'buenos', 'dias', 'buenas', 'tardes', 'buenas', 'noches', 'como', 'estas', 'hay', 'alguien', 'ahi', '?', 'hey', 'saludos', 'que', 'tal', 'chao', 'adios', 'hasta', 'luego', 'nos', 'vemos', 'bye', 'hasta', 'pronto', 'hasta', 'la', 'proxima', 'gracias', 'muchas', 'gracias', 'mil', 'gracias', 'muy', 'amable', 'se', 'lo', 'agradezco', 'fue', 'de', 'ayuda', 'gracias', 'por', 'la', 'ayuda', 'muy', 'agradecido', 'gracias', 'por', 'su', 'tiempo', 'ty']\n",
      "\n",
      " ####################################################################################################### \n",
      "\n",
      "[(['hola'], 'saludos'), (['buenos', 'dias'], 'saludos'), (['buenas', 'tardes'], 'saludos'), (['buenas', 'noches'], 'saludos'), (['como', 'estas'], 'saludos'), (['hay', 'alguien', 'ahi', '?'], 'saludos'), (['hey'], 'saludos'), (['saludos'], 'saludos'), (['que', 'tal'], 'saludos'), (['chao'], 'despedidas'), (['adios'], 'despedidas'), (['hasta', 'luego'], 'despedidas'), (['nos', 'vemos'], 'despedidas'), (['bye'], 'despedidas'), (['hasta', 'pronto'], 'despedidas'), (['hasta', 'la', 'proxima'], 'despedidas'), (['gracias'], 'agradecimientos'), (['muchas', 'gracias'], 'agradecimientos'), (['mil', 'gracias'], 'agradecimientos'), (['muy', 'amable'], 'agradecimientos'), (['se', 'lo', 'agradezco'], 'agradecimientos'), (['fue', 'de', 'ayuda'], 'agradecimientos'), (['gracias', 'por', 'la', 'ayuda'], 'agradecimientos'), (['muy', 'agradecido'], 'agradecimientos'), (['gracias', 'por', 'su', 'tiempo'], 'agradecimientos'), (['ty'], 'agradecimientos')]\n",
      "\n",
      " ####################################################################################################### \n",
      "\n",
      "['saludos', 'despedidas', 'agradecimientos']\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "classes=[]\n",
    "documents=[]\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        \n",
    "        \n",
    "        #tokenizar cada palabra\n",
    "        \n",
    "        w=nltk.word_tokenize(pattern) #separamos las oraciones palabra por palabra y cuardamos cada palabra como token\n",
    "        words.extend(w)\n",
    "        \n",
    "        #agrego un array de documentos\n",
    "        documents.append((w,intent[\"tag\"]))\n",
    "        \n",
    "        #añadimos clases  a nuestra lista de clases\n",
    "        if intent[\"tag\"] not in classes:\n",
    "            classes.append(intent[\"tag\"])\n",
    "            \n",
    "print(words)\n",
    "\n",
    "print(\"\\n ####################################################################################################### \\n\")\n",
    "\n",
    "print(documents)\n",
    "\n",
    "print(\"\\n ####################################################################################################### \\n\")\n",
    "\n",
    "print(classes)\n",
    "\n",
    "\n",
    "\n",
    "#recorderis\n",
    "#append añade todo en el mismo formato, ejemplo si append([lista]) lo agrega como lista\n",
    "#extend añade los elementos uno por uno , ejemplo si extend([lista]) agrega cada elemento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf642108",
   "metadata": {},
   "source": [
    "Ahora lematizaremos cada palabra y eliminaremos las palabras duplicadas de la lista.\n",
    "Lematizar es el proceso de convertir una palabra en su forma de lema y luego crear un archivo pickle para almacenar\n",
    "los objetos de Python que usaremos al predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11d4300b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'buenos', 'dia', 'buenas', 'tardes', 'buenas', 'noches', 'como', 'estas', 'hay', 'alguien', 'ahi', 'hey', 'saludos', 'que', 'tal', 'chao', 'adios', 'hasta', 'luego', 'no', 'vemos', 'bye', 'hasta', 'pronto', 'hasta', 'la', 'proxima', 'gracias', 'muchas', 'gracias', 'mil', 'gracias', 'muy', 'amable', 'se', 'lo', 'agradezco', 'fue', 'de', 'ayuda', 'gracias', 'por', 'la', 'ayuda', 'muy', 'agradecido', 'gracias', 'por', 'su', 'tiempo', 'ty']\n"
     ]
    }
   ],
   "source": [
    "# w.replace(é, e) # pendiente\n",
    "\n",
    "#base de datos, verdad absoluta de las palabras -> REFERENCIA\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words  if w not in ignore_words ]\n",
    "\n",
    "\n",
    "#for w in words:\n",
    "#    if w not in ignore_words:          # lo mismo de arriba escrito de otra manera\n",
    "#        word=lemmatizer.lemmatize(w.lower())\n",
    "    \n",
    "print(words)\n",
    "\n",
    "pickle.dump(words,open(\"words.pkl\",\"wb\"))\n",
    "pickle.dump(classes,open(\"classes.pkl\",\"wb\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7205b",
   "metadata": {},
   "source": [
    "### teoria\n",
    "\n",
    "https://www.nltk.org/book_1ed/ch03.html\n",
    "\n",
    "   Method               Functionality\n",
    "- s.find(t)=\t         index of first instance of string t inside s (-1 if not found)\n",
    "- s.rfind(t)=\t     index of last instance of string t inside s (-1 if not found)\n",
    "- s.index(t)=\t     like s.find(t) except it raises ValueError if not found\n",
    "- s.rindex(t)=\t     like s.rfind(t) except it raises ValueError if not found\n",
    "- s.join(text)=\t     combine the words of the text into a string using s as the glue\n",
    "- s.split(t)=\t     split s into a list wherever a t is found (whitespace by default)\n",
    "- s.splitlines()=\t split s into a list of strings, one per line\n",
    "- s.lower()=\t         a lowercased version of the string s\n",
    "- s.upper()=\t         an uppercased version of the string s\n",
    "- s.title()=\t         a titlecased version of the string s\n",
    "- s.strip()=\t         a copy of s without leading or trailing whitespace\n",
    "- s.replace(t, u)=\t replace instances of t with u inside s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662debb",
   "metadata": {},
   "source": [
    "Ahora, crearemos los datos de entrenamiento en los que proporcionaremos la entrada y la salida.\n",
    "Nuestra entrada será el patrón y la salida será la clase a la que pertenece nuestro patrón de entrada. \n",
    "Pero la computadora no entiende el texto, así que convertiremos el texto en números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19dd3232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "[[list([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0])]\n",
      " [list([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0])]\n",
      " [list([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0])]\n",
      " [list([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "  list([0, 0, 1])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "  list([0, 0, 1])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "  list([0, 0, 1])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 0, 1])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 0, 1])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 0, 1])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0])\n",
      "  list([0, 0, 1])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "  list([0, 0, 1])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0])\n",
      "  list([0, 0, 1])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "  list([0, 0, 1])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joseb\\AppData\\Local\\Temp\\ipykernel_5720\\4292130521.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training=np.array(training)\n"
     ]
    }
   ],
   "source": [
    "# preparacion para la formacion de nuestra red neuronal \n",
    "\n",
    "\n",
    "#en esta primera parte de nuestra red neuronal estamos haciendo un clasificador, words en tags\n",
    "\n",
    "training=[]\n",
    "output_empty=[0]*len(classes)# creamos una matriz del numero de patterns con valor inicial 0\n",
    "\n",
    "for doc in documents: #en doc esta la raw_data -> datos sin procesar\n",
    "    \n",
    "    #bag of words\n",
    "    bag=[]\n",
    "    #lista de tokens\n",
    "    pattern_words=doc[0]# doc[0] es la lista de palabras\n",
    "    # lematizacion del token\n",
    "    pattern_words= [lemmatizer.lemmatize(word.lower()) for word in pattern_words  if word not in ignore_words ]\n",
    "    # si la palabra coincide introduzco 1, en caso contrario 0\n",
    "    \n",
    "    for palabra in words:\n",
    "        bag.append(1) if palabra in pattern_words else bag.append(0) \n",
    "    \n",
    "    output_row =list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1 #doc en la posicion 1 es el pattern\n",
    "    \n",
    "    training.append([bag,output_row])\n",
    "    #print(output_row)\n",
    "\n",
    "training=np.array(training)\n",
    "\n",
    "print(len(training[0][0]))\n",
    "print(training)\n",
    "\n",
    " \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe26cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#que hace todo este codigo anterior tan horroroso:\n",
    "# estamos ENCRIPTANDO\n",
    "\n",
    "convierte palabras en \"booleano\"(lista de 1 y 0) y les asigna una categoria \n",
    "\n",
    "para asignal la categoria busca si esta en la lista de referencia\n",
    "\n",
    "como son las categorias:\n",
    "    [1,0,0]=saludos\n",
    "    [0,1,0]=despedidas\n",
    "    [0,0,1]=agradecimientos\n",
    "\n",
    "    \n",
    "forma pares:    \n",
    "\n",
    "hola   :[1, 0, 0, 0, 0, 0, ... [1,0,0] -> 10000 : saludo\n",
    "buenos :[0, 1, 1, 0, 0, 0, ... [1,0,0] -> 01100 : saludo\n",
    "         \n",
    "estos pares son la data que vamos a mandar a la IA para poder crear el modelo\n",
    " \n",
    "         \n",
    " data=entradas     target=salidas\n",
    "c1,c2,c3,c4,c5,c6...s1,s2,s3\n",
    "1  0  0  0  0  0    1 0  0  -> saludo    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

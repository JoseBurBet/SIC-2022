{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dfa2b6",
   "metadata": {},
   "source": [
    "# Primeras nociones de deep-learning y redes neuronales \n",
    "## Ejemplo chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bd059",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip list #para saber que tengo instalado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab544c10",
   "metadata": {},
   "source": [
    "import pickle  #https://docs.python.org/es/3/library/pickle.html  # guardar modelos (o objetos) como secuencia de bytes (unos y ceros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9b61c",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "\n",
    "- que es en palabras sencillas\n",
    " https://www.incentro.com/es-ES/blog/que-es-tensorflow\n",
    " \n",
    "- link de la libreria\n",
    " https://www.tensorflow.org/?gclid=Cj0KCQiA4OybBhCzARIsAIcfn9nLmib7_Bs2vXmVsx1nI5y3gF07NhydNa0Tfgf6rxdBTf1zZJiA08kaAt7XEALw_wcB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c4e9a",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras\n",
    "\n",
    "- tensorflow vs keras, que son?\n",
    "https://nodd3r.com/blog/keras-vs-tensorflow-vs-pytorch-diferencias-clave-entre-frameworks-de-deep-learning\n",
    "\n",
    "-   que es una api\n",
    "https://www.sydle.com/es/blog/api-6214f68876950e47761c40e7/\n",
    "Una API es una especie de puente que conecta diversos tipos de software o aplicaciones y puede crearse en varios lenguajes de programación. Además de un buen desarrollo, una API debe tener una documentación clara y objetiva para poder facilitar su implementación.\n",
    "\n",
    "-  link libreria\n",
    "https://keras.io/examples/   ESTO LES SIRVE PARA LOS PROYECTOS !!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b0f51",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "\n",
    "- que es en palabras sencillas \n",
    "https://code.tutsplus.com/es/tutorials/introducing-the-natural-language-toolkit-nltk--cms-28620\n",
    "\n",
    "- link de la libreria\n",
    "https://www.nltk.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ad5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('spanish_grammars')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('stopwords') # faltaaaaa\n",
    "nltk.download('omw-1.4') #faltaaaa\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0abeb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def guardar_json(datos):\n",
    "    archivo=open(\"intents.json\",\"w\")\n",
    "    json.dump(datos,archivo,indent=4)\n",
    "\n",
    "#intents: grupos de conversaciones tipicas para nuestro objetivo\n",
    "# patterns: posibles interacciones con el usuario\n",
    "\n",
    "#dic={\"intents:[[{\"key\":[\"valores\"]}],\"dic2\"]}\n",
    "\n",
    "\n",
    "biblioteca={\"intents\":\n",
    "            [\n",
    "                {\"tag\":\"saludos\",\n",
    "                 \"patterns\":[\"hola\",\n",
    "                             \"buenos dias\",\n",
    "                             \"buenas tardes\",\n",
    "                             \"buenas noches\",\n",
    "                             \"como estas\",\n",
    "                             \"hay alguien ahi?\",\n",
    "                             \"hey\",\n",
    "                             \"saludos\",\n",
    "                             \"que tal\"                      \n",
    "                             ],\n",
    "                 \"responses\":[\"hola soy SIC-BOT , en que puedo ayudarte\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]\n",
    "                 },\n",
    "                \n",
    "                {\"tag\":\"despedidas\",\n",
    "                 \"patterns\":[\"chao\",\n",
    "                             \"adios\",\n",
    "                             \"hasta luego\",\n",
    "                             \"nos vemos\",\n",
    "                             \"bye\",\n",
    "                             \"hasta pronto\",\n",
    "                             \"hasta la proxima\"\n",
    "                             ],\n",
    "                 \"responses\":[\"hasta luego, tenga un buen dia\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]\n",
    "                 },\n",
    "                {\"tag\":\"agradecimientos\",\n",
    "                 \"patterns\":[\"gracias\",\n",
    "                             \"muchas gracias\",\n",
    "                             \"mil gracias\",\n",
    "                             \"muy amable\",\n",
    "                             \"se lo agradezco\",\n",
    "                             \"fue de ayuda\",\n",
    "                             \"gracias por la ayuda\",\n",
    "                             \"muy agradecido\",\n",
    "                             \"gracias por su tiempo\",\n",
    "                             \"ty\"\n",
    "                            ],\n",
    "                 \"responses\":[\"de nada\",\n",
    "                              \"feliz por ayudarlo\",\n",
    "                              \"gracias a usted\",\n",
    "                              \"estamos para servirle\",\n",
    "                              \"fue un placer\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]\n",
    "                },\n",
    "                {\"tag\":\"norespuesta\",\n",
    "                 \"patterns\":[\"\"],\n",
    "                 \"responses\":[\"no se detecto una respuesta\"\n",
    "                             ],\n",
    "                 \"context\":[\"\"]                    \n",
    "                }\n",
    "            ] \n",
    "        }\n",
    "guardar_json(biblioteca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a56a78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'saludos',\n",
       "   'patterns': ['hola',\n",
       "    'buenos dias',\n",
       "    'buenas tardes',\n",
       "    'buenas noches',\n",
       "    'como estas',\n",
       "    'hay alguien ahi?',\n",
       "    'hey',\n",
       "    'saludos',\n",
       "    'que tal'],\n",
       "   'responses': ['hola soy SIC-BOT , en que puedo ayudarte'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'despedidas',\n",
       "   'patterns': ['chao',\n",
       "    'adios',\n",
       "    'hasta luego',\n",
       "    'nos vemos',\n",
       "    'bye',\n",
       "    'hasta pronto',\n",
       "    'hasta la proxima'],\n",
       "   'responses': ['hasta luego, tenga un buen dia'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'agradecimientos',\n",
       "   'patterns': ['gracias',\n",
       "    'muchas gracias',\n",
       "    'mil gracias',\n",
       "    'muy amable',\n",
       "    'se lo agradezco',\n",
       "    'fue de ayuda',\n",
       "    'gracias por la ayuda',\n",
       "    'muy agradecido',\n",
       "    'gracias por su tiempo',\n",
       "    'ty'],\n",
       "   'responses': ['de nada',\n",
       "    'feliz por ayudarlo',\n",
       "    'gracias a usted',\n",
       "    'estamos para servirle',\n",
       "    'fue un placer'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'norespuesta',\n",
       "   'patterns': [''],\n",
       "   'responses': ['no se detecto una respuesta'],\n",
       "   'context': ['']}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "ignore_words=[\"?\",\"¿\",\"!\",\"¡\"]\n",
    "data_file= open(\"intents.json\").read() #aqui cargo el archivo en formato json\n",
    "intents = json.loads(data_file) # aqui convierto el archivo json a diccionario\n",
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7d129",
   "metadata": {},
   "source": [
    "Cuando trabajamos con datos de texto, necesitamos realizar varios preprocesos en los datos antes de hacer un machine\n",
    "learning o un modelo de Deep learning.\n",
    "\n",
    "Según los requisitos, necesitamos aplicar varias operaciones para preprocesar los datos.\n",
    "\n",
    "La creación de tokens es lo más básico y lo primero que puedes hacer con los datos de texto. \n",
    "\n",
    "La creación de tokens es el proceso de dividir todo el texto en partes pequeñas, como palabras.\n",
    "\n",
    "Aquí iteramos a través de los patrones y tokenizamos la oración usando la función nltk.word_tokenize ()\n",
    "y agregamos cada palabra en la lista de palabras. También creamos una lista de clases para nuestras etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd877a3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'buenos', 'dias', 'buenas', 'tardes', 'buenas', 'noches', 'como', 'estas', 'hay', 'alguien', 'ahi', '?', 'hey', 'saludos', 'que', 'tal', 'chao', 'adios', 'hasta', 'luego', 'nos', 'vemos', 'bye', 'hasta', 'pronto', 'hasta', 'la', 'proxima', 'gracias', 'muchas', 'gracias', 'mil', 'gracias', 'muy', 'amable', 'se', 'lo', 'agradezco', 'fue', 'de', 'ayuda', 'gracias', 'por', 'la', 'ayuda', 'muy', 'agradecido', 'gracias', 'por', 'su', 'tiempo', 'ty']\n",
      "\n",
      " ####################################################################################################### \n",
      "\n",
      "[(['hola'], 'saludos'), (['buenos', 'dias'], 'saludos'), (['buenas', 'tardes'], 'saludos'), (['buenas', 'noches'], 'saludos'), (['como', 'estas'], 'saludos'), (['hay', 'alguien', 'ahi', '?'], 'saludos'), (['hey'], 'saludos'), (['saludos'], 'saludos'), (['que', 'tal'], 'saludos'), (['chao'], 'despedidas'), (['adios'], 'despedidas'), (['hasta', 'luego'], 'despedidas'), (['nos', 'vemos'], 'despedidas'), (['bye'], 'despedidas'), (['hasta', 'pronto'], 'despedidas'), (['hasta', 'la', 'proxima'], 'despedidas'), (['gracias'], 'agradecimientos'), (['muchas', 'gracias'], 'agradecimientos'), (['mil', 'gracias'], 'agradecimientos'), (['muy', 'amable'], 'agradecimientos'), (['se', 'lo', 'agradezco'], 'agradecimientos'), (['fue', 'de', 'ayuda'], 'agradecimientos'), (['gracias', 'por', 'la', 'ayuda'], 'agradecimientos'), (['muy', 'agradecido'], 'agradecimientos'), (['gracias', 'por', 'su', 'tiempo'], 'agradecimientos'), (['ty'], 'agradecimientos'), ([], 'norespuesta')]\n",
      "\n",
      " ####################################################################################################### \n",
      "\n",
      "['saludos', 'despedidas', 'agradecimientos', 'norespuesta']\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "classes=[]\n",
    "documents=[]\n",
    "\n",
    "for intent in intents[\"intents\"]: #accedo a la lista de diccionarios\n",
    "    for pattern in intent[\"patterns\"]: # accedo a la lista de palabraas\n",
    "        \n",
    "        \n",
    "        #tokenizar cada palabra\n",
    "        \n",
    "        w=nltk.word_tokenize(pattern) #separamos las oraciones palabra por palabra y guardamos cada palabra como token\n",
    "        words.extend(w)\n",
    "        \n",
    "        #agrego un array de documentos\n",
    "        documents.append((w,intent[\"tag\"]))\n",
    "        \n",
    "        #añadimos clases  a nuestra lista de clases\n",
    "        if intent[\"tag\"] not in classes:\n",
    "            classes.append(intent[\"tag\"])\n",
    "            \n",
    "print(words)\n",
    "\n",
    "print(\"\\n ####################################################################################################### \\n\")\n",
    "\n",
    "print(documents)\n",
    "\n",
    "print(\"\\n ####################################################################################################### \\n\")\n",
    "\n",
    "print(classes)\n",
    "\n",
    "\n",
    "\n",
    "#recorderis\n",
    "#append añade todo en el mismo formato, ejemplo si append([lista]) lo agrega como lista\n",
    "#extend añade los elementos uno por uno , ejemplo si extend([lista]) agrega cada elemento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf642108",
   "metadata": {},
   "source": [
    "Ahora lematizaremos cada palabra y eliminaremos las palabras duplicadas de la lista.\n",
    "Lematizar es el proceso de convertir una palabra en su forma de lema y luego crear un archivo pickle para almacenar\n",
    "los objetos de Python que usaremos al predecir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf16ef4",
   "metadata": {},
   "source": [
    "# CAMBIO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9711bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esta lista words seran las palabras LEMATIZADAS de REFERENCIA\n",
      "['hol', 'buen', 'dia', 'buen', 'tard', 'buen', 'noch', 'com', 'estas', 'hay', 'algui', 'ahi', 'hey', 'salud', 'que', 'tal', 'cha', 'adi', 'hast', 'lueg', 'no', 'vem', 'bye', 'hast', 'pront', 'hast', 'la', 'proxim', 'graci', 'much', 'graci', 'mil', 'graci', 'muy', 'amabl', 'se', 'lo', 'agradezc', 'fue', 'de', 'ayud', 'graci', 'por', 'la', 'ayud', 'muy', 'agradec', 'graci', 'por', 'su', 'tiemp', 'ty']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# ejemplo de lematizacion: https://es.wikipedia.org/wiki/Lematización\n",
    "# lema -> palabra completa  : categoria\n",
    "# casa -> casas             : plurales\n",
    "# dije -> diré,dijeramos    :tiempos verbales\n",
    "\n",
    "#La lematización es el proceso de agrupar las diferentes formas flexionadas de una \n",
    "                                #palabra para que puedan ser analizadas como un único elemento\n",
    "\n",
    "\n",
    "#base de datos, verdad absoluta de las palabras -> REFERENCIA\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "\n",
    "print(\"esta lista words seran las palabras LEMATIZADAS de REFERENCIA\")\n",
    "print(words)\n",
    "\n",
    "\n",
    "pickle.dump(words,open(\"words.pkl\",\"wb\"))\n",
    "pickle.dump(classes,open(\"classes.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7205b",
   "metadata": {},
   "source": [
    "### teoria\n",
    "\n",
    "https://www.nltk.org/book_1ed/ch03.html\n",
    "\n",
    "   Method               Functionality\n",
    "- s.find(t)=\t         index of first instance of string t inside s (-1 if not found)\n",
    "- s.rfind(t)=\t     index of last instance of string t inside s (-1 if not found)\n",
    "- s.index(t)=\t     like s.find(t) except it raises ValueError if not found\n",
    "- s.rindex(t)=\t     like s.rfind(t) except it raises ValueError if not found\n",
    "- s.join(text)=\t     combine the words of the text into a string using s as the glue\n",
    "- s.split(t)=\t     split s into a list wherever a t is found (whitespace by default)\n",
    "- s.splitlines()=\t split s into a list of strings, one per line\n",
    "- s.lower()=\t         a lowercased version of the string s\n",
    "- s.upper()=\t         an uppercased version of the string s\n",
    "- s.title()=\t         a titlecased version of the string s\n",
    "- s.strip()=\t         a copy of s without leading or trailing whitespace\n",
    "- s.replace(t, u)=\t replace instances of t with u inside s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662debb",
   "metadata": {},
   "source": [
    "Ahora, crearemos los datos de entrenamiento en los que proporcionaremos la entrada y la salida.\n",
    "Nuestra entrada será el patrón y la salida será la clase a la que pertenece nuestro patrón de entrada. \n",
    "Pero la computadora no entiende el texto, así que convertiremos el texto en números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19dd3232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tendremos como entradas 52 columnas\n",
      "tendremos como salidas 4 columnas\n",
      "\n",
      "[[list([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0, 0])]\n",
      " [list([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0, 0])]\n",
      " [list([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0, 0])]\n",
      " [list([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([1, 0, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 1, 0, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "  list([0, 0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "  list([0, 0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "  list([0, 0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0])\n",
      "  list([0, 0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "  list([0, 0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0])\n",
      "  list([0, 0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "  list([0, 0, 1, 0])]\n",
      " [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  list([0, 0, 0, 1])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joseb\\AppData\\Local\\Temp\\ipykernel_2696\\414362171.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training=np.array(training) # cambiamos la lista de listas a un formato numpy.array\n"
     ]
    }
   ],
   "source": [
    "# preparacion para la formacion de nuestra red neuronal \n",
    "\n",
    "\n",
    "#en esta primera parte de nuestra red neuronal estamos haciendo un clasificador, words en tags\n",
    "\n",
    "training=[]\n",
    "output_empty=[0]*len(classes)# creamos una matriz del numero de patterns con valor inicial 0\n",
    "                            # creamos una matriz que tenga tantas columnas como classes\n",
    "\n",
    "for doc in documents: #en doc esta la raw_data -> datos sin procesar\n",
    "#doc[0] -> tokens, o palabras\n",
    "#doc[1] -> tag -> clase\n",
    "    \n",
    "    \n",
    "    #bag of words\n",
    "    bag=[]\n",
    "    #lista de tokens\n",
    "    pattern_words=doc[0]# doc[0] es la lista de palabras\n",
    "    # lematizacion del token\n",
    "#CAMBIO!!!!    pattern_words= [lemmatizer.lemmatize(word.lower()) for word in pattern_words  if word not in ignore_words ]\n",
    "    pattern_words= [stemmer.stem(word.lower()) for word in pattern_words  if word not in ignore_words ]\n",
    "    \n",
    "    \n",
    "    # si la palabra coincide introduzco 1, en caso contrario 0\n",
    "    \n",
    "    for palabra in words:\n",
    "        bag.append(1) if palabra in pattern_words else bag.append(0) \n",
    "        #si la palabra de referencia esta dentro de pattern_words ponga 1\n",
    "        #print(bag)\n",
    "    \n",
    "    output_row =list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1 #doc en la posicion 1 es el pattern\n",
    "                #busca en que posicion esta el tag y pone un 1 en esa posicion del output_row\n",
    "                #ejemplo si es saludo pone [1,0,0,0]\n",
    "    \n",
    "    training.append([bag,output_row])\n",
    "    #print(output_row)\n",
    "\n",
    "#print(training)\n",
    "training=np.array(training) # cambiamos la lista de listas a un formato numpy.array\n",
    "\n",
    "print(f\"tendremos como entradas {len(training[0][0])} columnas\")\n",
    "#print(f\"tendremos como entradas {} columnas\".format(len(training[0][0])))      \n",
    "\n",
    "print(f\"tendremos como salidas {len(training[0][1])} columnas\")      \n",
    "print()      \n",
    "print(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "que hace todo este codigo anterior tan horroroso:\n",
    "- estamos ENCRIPTANDO palabras a listas de 1 y 0\n",
    "\n",
    "convierte palabras en \"booleano\"(lista de 1 y 0) y les asigna una categoria \n",
    "\n",
    "para asignal la categoria busca si esta en la lista de referencia\n",
    "\n",
    "como son las categorias:\n",
    "    [1,0,0,0]=saludos\n",
    "    [0,1,0,0]=despedidas\n",
    "    [0,0,1,0]=agradecimientos\n",
    "    [0,0,0,1]=norespuesta\n",
    "\n",
    "    \n",
    "forma pares:    \n",
    "\n",
    "hola   :[1, 0, 0, 0, 0, 0, ... [1,0,0] -> 10000 : saludo\n",
    "buenos :[0, 1, 1, 0, 0, 0, ... [1,0,0] -> 01100 : saludo\n",
    "         \n",
    "estos pares son la data que vamos a mandar a la IA para poder crear el modelo\n",
    " \n",
    "     52                  4\n",
    " data=entradas     target=salidas\n",
    "c1,c2,c3,c4,c5,c6...s1,s2,s3,s4\n",
    "1  0  0  0  0  0    1  0  0   0  -> saludo    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3155f9",
   "metadata": {},
   "source": [
    "# Hasta ahi la clase anterior con los cambios en el lematizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12aecb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en la clase anterior habiamos creado el training, ahora separamos\n",
    "# crear los conjuntos de entrenamiento  y prueba, x_test, x_train (entradas) , y_test , y_train (salidas)\n",
    "        \n",
    "#x_train= training[0] asi seria en listas     \n",
    "x_train= list(training[:,0]) #asi porque estamos en formato numpy.array ||| training[inicio:fin,index]\n",
    "y_train= list(training[:,1])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ca5fa",
   "metadata": {},
   "source": [
    "Tenemos nuestros datos de entrenamiento listos, ahora construiremos una red neuronal profunda que tiene 3 capas.\n",
    "https://www.youtube.com/watch?v=IQMoglp-fBk # como funciona una red neuronal de 3 capas\n",
    "\n",
    "Usamos la API secuencial de Keras para esto.\n",
    "Después de entrenar el modelo durante 200 épocas, logramos una precisión del 100% en nuestro modelo.\n",
    "Guardemos el modelo como “chatbot_model.h5”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
